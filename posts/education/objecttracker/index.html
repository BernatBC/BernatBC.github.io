<!doctype html><html lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Object Tracker - BernatBC</title>
<link rel=stylesheet href=https://www.bernatbc.tk/styles/style.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css integrity=sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js integrity=sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload=renderMathInElement(document.body)></script></head><body class=dark-theme><div class=page-wrapper><nav class=navbar><div class=navbar-container><button id=navbar-toggle class=navbar-toggle>
<i class="fas fa-bars"></i></button><div class=navbar-nav id=navbar-nav><ul><li><a href=/#profile class=nav-link><i class="fas fa-user"></i> Profile</a></li><li><a href=/#skills class=nav-link><i class="fas fa-code"></i> Skills</a></li><li><a href=/#experiences class=nav-link><i class="fas fa-briefcase"></i> Experiences</a></li><li><a href=/#education class=nav-link><i class="fas fa-graduation-cap"></i> Education</a></li><li><a href=/#projects class=nav-link><i class="fas fa-project-diagram"></i> Open Source Projects</a></li><li><a href=/#recent-posts class=nav-link><i class="fas fa-newspaper"></i> Recent Posts</a></li><li><a href=/posts class="nav-link active"><i class="fas fa-pen"></i> All Posts</a></li></ul></div></div></nav><div class=content-wrapper><aside class=toc-container><div class=toc><h3 class=toc-title><i class="fas fa-list"></i> Table of Contents</h3><div id=toc-content class=toc-content></div></div></aside><div class=main-content><div class=container><article class=post-single><header class=post-header><h1 class=section-title>Object Tracker</h1><div class=post-meta><div class=post-meta-top><span class=post-date><i class="far fa-calendar-alt"></i> June 23, 2023</span></div><div class=post-tags><span class=post-tag>AI</span>
<span class=post-tag>Computer Vision</span>
<span class=post-tag>Matlab</span>
<span class=post-tag>Python</span>
<span class=post-tag>Education</span></div></div><div class=post-featured-image><img src=/posts/education/objecttracker/hero.png alt="Object Tracker"></div></header><div class=post-content><h2 id=the-statement>The statement</h2><p><a href=https://amoudgl.github.io/tlp/>Long-Term Visual Object Tracking Benchmark</a> is a dataset to benchmark object tracking algorithms. We were asked to implement four different algorithms. Two using tracking techniques, and two using recognition. For each technique, we had to implement one from scratch (an algorithm made for us) and another one extracted from the internet.</p><p>Given a series of frames and a bounding box corresponding to the object to track, the algorithm should follow the object. To evaluate the precision, we used the overlapping ratio between the computed bounding box and the optimal bounding box (which was given via a txt file).</p><p>The full statement is in the document below. Note that the statement is in Catalan.</p><embed src=ShortProjectQP2023.pdf width=100% height=700 type=application/pdf><h2 id=algorithms-and-methods>Algorithms and methods</h2><p>The four algorithms are the following:</p><h3 id=tracking-by-histogram-comparison>Tracking by histogram comparison</h3><p>The idea of this method is to find the most similar crop of the whole frame. To achieve it, we decided to compute the distance between the color histograms of both crops. Normalized color histograms are a 2D matrix where each cell represents an interval of color proportion from each component of RGB. One axis belongs to the red component, and the other one belongs to the green component. The blue component can be found by B = 1 - R - G.</p><p>We decided to use the <a href=https://en.wikipedia.org/wiki/Chi-squared_test>Chi-squared</a> distance between two histograms, as it is quite efficient and returns a pretty good value.</p><p>Obviously, this method is not efficient as it needs to compute 1280×720 possible crops. We had to reduce the number of comparisons, sacrificing precision. The first optimization was to only look for crops near the position of the object from the last frame. We decided to only check for crops that overlapped the previous bounding box. We accomplished drastically reducing the number of comparisons, but it wasn&rsquo;t enough. The second optimization was about doing comparisons every certain number of pixels. For example, if we checked every 5 pixels, we would reduce the number of comparisons by 25 times. We found that we could afford to check more frequently on smaller bounding boxes, so the value of the interval is computed depending on the size.</p><p>As you can read in the report, this was the best tracking method, as it consistently tracked the object, even though sometimes the precision is not there. Also, it&rsquo;s the second-best method after <code>Recognition using Yolov8 trained by ourselves</code>.</p><h3 id=tracking-by-sift-and-ransac>Tracking by SIFT and RANSAC</h3><p>The idea of this method is to find keypoints and their descriptors using the <a href=https://en.wikipedia.org/wiki/Harris_corner_detector>Harris corner detector</a> and the<a href=https://en.wikipedia.org/wiki/Scale-invariant_feature_transform>Scale-invariant feature transform (SIFT)</a> from both the crop of the object and the whole frame. After the keypoints and their descriptors are extracted, points are matched by distance, and we obtain the best matching using the <a href=https://en.wikipedia.org/wiki/Random_sample_consensus>Random sample consensus (RANSAC)</a>.</p><p>The results of this method weren&rsquo;t what we expected. This was the one that performed the worst, and by quite a bit. We detected two major faults. The first one is that if the object is not detailed, we couldn&rsquo;t find many keypoints, leading to a low number of matchings. RANSAC needs at least 3 pairs in order to compute the best matching. In some cases, even 3 pairs weren&rsquo;t enough, so <a href=https://en.wikipedia.org/wiki/Random_sample_consensus>RANSAC</a> could not return a matching. The other fault was if the background was detailed and weighted. The algorithm detected many points in the background, and the matching produced was always made in the starting position (the algorithm was matching the background but not the object).</p><h3 id=recognition-using-yolov8httpsdocsultralyticscom>Recognition using <a href=https://docs.ultralytics.com/>Yolov8</a></h3><p>We found a neural network named <a href=https://docs.ultralytics.com/>Yolov8</a>, that could detect several objects in an image. This network includes 80 different types of objects that can be distinguished. We just used it, but then we had the issue that we had to filter the results obtained. We manually selected the type of object to detect, and in some cases, more than one instance was recognized. Likewise, we made the decision to select the object that had the greatest confidence level.</p><p>The results of this method were pretty good, but a little lower than <code>Tracking by histogram comparison</code> and <code>Recognition using Yolov8 trained by ourselves</code>. The main issue was that there were a couple objects that the network didn&rsquo;t support, like <code>fish</code> and <code>faces</code>. Another issue found was not from the neural network; it was from our &lsquo;selection method&rsquo; when we detected multiple instances. An example could be found in the video <code>Boxing2</code>. We had to detect a certain boxer, and even though the network recognized it, it also recognized the opponent, and with a greater confidence level.</p><h3 id=recognition-using-yolov8httpsdocsultralyticscom-trained-by-ourselves>Recognition using <a href=https://docs.ultralytics.com/>Yolov8</a> trained by ourselves</h3><p>In this method, we decided to train <a href=https://docs.ultralytics.com/>Yolov8</a> with our own dataset. The professor allowed us to &ldquo;cheat&rdquo;, and add crops of the videos in order to train the network. We added 5 crops from each object to detect to our neural network, and after some failures, we got a pretty good recognition network for our videos.</p><p>Obviously, the results were the best ones, but we have to consider that when using it in other samples, the results will not be the ones we got. This network is specifically trained to recognize objects from this dataset only.</p><h2 id=getting-started>Getting Started</h2><h3 id=tracking-by-histogram-comparison-1>Tracking by histogram comparison</h3><h4 id=1-install-dependencies>1. Install dependencies</h4><p>The following dependencies are required:</p><ul><li>Matlab (latest version not necessary but recommended)</li></ul><h4 id=2-get-the-source-code>2. Get the source code</h4><p>To get the source code, you can simply download the zip file, or you can clone this repository by typing:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/BernatBC/ObjectTracker.git
</span></span></code></pre></div><h4 id=3-launch-matlab-and-run-code>3. Launch Matlab and run code</h4><p>You&rsquo;ll need to execute Matlab. Then you just open <code>tracking_nostre.mlx</code>, and press the Button Run.</p><p>Make sure you have <code>TinyTLP</code> folder in the same directory as the <code>tracking_nostre.mlx</code>.</p><h3 id=tracking-by-sift-and-ransac-1>Tracking by SIFT and RANSAC</h3><p>You need to follow the same steps as <code>Tracking by histogram comparison</code>, but with the source code file <code>tracking_internet.mlx</code>.</p><h3 id=recognition-using-yolov8httpsdocsultralyticscom-1>Recognition using <a href=https://docs.ultralytics.com/>Yolov8</a></h3><h4 id=1-install-dependencies-1>1. Install dependencies</h4><p>The following dependencies are required:</p><ul><li>python3</li><li>pip</li></ul><p>After the installation of those packages, we&rsquo;ll install the following python packages:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install ultralytics
</span></span></code></pre></div><h4 id=2-get-the-source-code-1>2. Get the source code</h4><p>To get the source code, you can simply download the zip file, or you can clone this repository by typing:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/BernatBC/ObjectTracker.git
</span></span></code></pre></div><h4 id=3-run-python-script>3. Run python script</h4><p>From the repository directory, run the following command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python3 reconeixament_internet.py
</span></span></code></pre></div><p>Make sure you have <code>TinyTLP</code> folder in the same directory as the <code>reconeixament_internet.py</code>.</p><h3 id=recognition-using-yolov8httpsdocsultralyticscom-trained-by-ourselves-1>Recognition using <a href=https://docs.ultralytics.com/>Yolov8</a> trained by ourselves</h3><p>You need to follow the steps 1 and 2 from <code>Recognition using Yolov8</code>.</p><h4 id=3-run-python-script-1>3. Run python script</h4><p>From the repository directory, run the following command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python3 reconeixament_nostre.py
</span></span></code></pre></div><p>Make sure you have <code>TinyTLP</code> folder in the same directory as the <code>reconeixament_nostre.py</code>.</p><h3 id=view-code-and-report-on-i-classfab-fa-githubigithubhttpsgithubcombernatbcobjecttracker><a href=https://github.com/bernatbc/objecttracker>View Code and report on <i class="fab fa-github"></i>Github</a></h3></div><div class=post-footer><div class=post-navigation><a href=/posts/education/lambdacalculustelegrambot/ class=prev-post><i class="fas fa-arrow-left"></i>
<span><small>Previous</small>
<strong>λ-Calculus Telegram Bot</strong>
</span></a><a href=/posts/contests/ncpc2023/ class=next-post><span><small>Next</small>
<strong>Nordic Collegiate Programming Contest 2023</strong>
</span><i class="fas fa-arrow-right"></i></a></div><a href=/posts class=back-to-posts><i class="fas fa-th-list"></i> Back to all posts</a></div></article></div><footer class=footer><div class=footer-contact><a href=bernat.borrasc@gmail.com class=footer-link title=Email><i class="fas fa-envelope"></i>
</a><a href=https://github.com/bernatbc class=footer-link title=Github><i class="fab fa-github"></i>
</a><a href=https://www.youtube.com/channel/UCWuKGhwo6zB_ptObwrumZXw class=footer-link title=Youtube><i class="fa-brands fa-youtube"></i>
</a><a href=https://www.linkedin.com/in/bernat-borr%c3%a0s-i-civil/ class=footer-link title=LinkedIn><i class="fab fa-linkedin"></i>
</a><a href=https://devpost.com/bernat-borras-civil class=footer-link title=Devpost><i class="fa-solid fa-d"></i></a></div><div class=footer-copyright><p>© 2025 Copyright.</p><p>Powered by <a href=https://gohugo.io target=_blank>gohugo</a></p></div></footer></div></div></div><script>document.addEventListener("DOMContentLoaded",function(){const s=document.getElementById("navbar-toggle"),o=document.getElementById("navbar-nav");s.addEventListener("click",function(){o.classList.toggle("show")}),document.querySelectorAll("pre > code").forEach(function(e){const t=document.createElement("button");t.className="copy-code-button",t.type="button",t.innerHTML='<i class="fas fa-copy"></i> Copy';const s=e.parentNode,n=document.createElement("div");n.className="code-block-wrapper";const o=e.className;if(o&&o.startsWith("language-")){const e=o.replace("language-","");n.setAttribute("data-lang",e)}s.parentNode.insertBefore(n,s),n.appendChild(s),n.appendChild(t),t.addEventListener("click",function(){navigator.clipboard.writeText(e.textContent).then(function(){t.innerHTML='<i class="fas fa-check"></i> Copied!',setTimeout(function(){t.innerHTML='<i class="fas fa-copy"></i> Copy'},2e3)},function(){t.innerHTML='<i class="fas fa-times"></i> Error',setTimeout(function(){t.innerHTML='<i class="fas fa-copy"></i> Copy'},2e3)})})});const e=document.querySelector(".post-content"),t=document.getElementById("toc-content");if(e&&t){const n=e.querySelectorAll("h2, h3, h4");if(n.length>0){const e=document.createElement("ul");n.forEach((t,n)=>{t.id||(t.id="heading-"+n);const o=document.createElement("li");o.className="toc-"+t.tagName.toLowerCase();const s=document.createElement("a");s.href="#"+t.id,s.textContent=t.textContent,o.appendChild(s),e.appendChild(o),s.addEventListener("click",function(e){e.preventDefault(),document.getElementById(t.id).scrollIntoView({behavior:"smooth"}),history.pushState(null,null,"#"+t.id)})}),t.appendChild(e)}else document.querySelector(".toc-container").style.display="none"}const n=document.querySelectorAll(".toc-content a");n.length&&window.addEventListener("scroll",function(){let e=null;const t=window.scrollY;document.querySelectorAll(".post-content h1, .post-content h2, .post-content h3, .post-content h4").forEach(n=>{const s=n.offsetTop;s-100<=t&&(e=document.querySelector(`.toc-content a[href="#${n.id}"]`))}),n.forEach(e=>{e.classList.remove("active")}),e&&e.classList.add("active")})})</script></body></html>